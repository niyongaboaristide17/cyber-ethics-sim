# Configuration for MongoDB to Elasticsearch data pipeline

# Input section: Configure MongoDB as the data source
input {
  mongodb {
    # MongoDB connection string with database name
    uri => 'mongodb://mongo:27017/cyber_ethics'
    
    # Directory and file for storing sync status
    placeholder_db_dir => '/usr/share/logstash/mongodb_placeholders'
    placeholder_db_name => 'logstash_sqlite.db'
    
    # MongoDB collection to read from
    collection => 'scenarios'
    
    # Number of documents to fetch in each batch
    batch_size => 500
    
    # Simple parsing method for MongoDB documents
    parse_method => 'simple'
    
    # Use JSON codec for data parsing
    codec => "json"
  }
}

# Filter section: Transform and modify the data
filter {
  mutate {
    # Rename MongoDB's _id to avoid conflicts with Elasticsearch
    rename => { "_id" => "mongo_id" }
    
    # Type conversion for numeric and boolean fields
    convert => {
      "views" => "integer"
      "likes" => "integer"
      "published" => "boolean"
      "isAIgenerated" => "boolean"
      "isFeatured" => "boolean"
      "editable" => "boolean"
    }
  }

  # Process the decisions array if present
  if [decisions] {
    ruby {
      # Extract option texts from decisions array for better searchability
      code => "
        event.set('decision_texts', event.get('decisions').map { |d| d['optionText'] })
      "
    }
  }
}

# Output section: Configure data destinations
output {
  # Primary output to Elasticsearch
  elasticsearch {
    # Elasticsearch cluster endpoint
    hosts => ["http://elasticsearch:9200"]
    
    # Target index name
    index => "scenarios"
    
    # Use MongoDB ID as Elasticsearch document ID
    document_id => "%{mongo_id}"
  }

  # Debug output to console
  stdout {
    codec => rubydebug
  }
}